{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "022546da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dependencies\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import entropy\n",
    "import spacy\n",
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect import detect_langs\n",
    "import fastlang\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "46efa110",
   "metadata": {},
   "outputs": [],
   "source": [
    "isbn = 9780060935467\n",
    "url = f\"https://openlibrary.org/search.json?q=isbn:{isbn}&fields=title,isbn,author_name,subject\"\n",
    "tags = requests.get(url).json()['docs'][0]['subject']\n",
    "selected_tags = [tags[i] for i in [3, 4, 5, 6, 7, 8, 12, 13, 15, 16]]\n",
    "print(selected_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8de9a28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/markayiah/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'racial segregation': 'Discipline', 'mob mentality': 'Diversity', 'Southern Gothic': 'Discipline', 'southern life': 'Discipline', 'racial injustice': 'Discipline', 'class': 'Diversity', 'laws': 'Discipline', 'loss of innocence': 'Diversity', 'domestic fiction': 'Diversity', 'legal stories': 'Discipline'}\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model for embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Desired diversity area and overarching discipline\n",
    "desired_diversity = 'gender representation'\n",
    "overarching_discipline = 'african studies'\n",
    "\n",
    "# Function to categorize tags\n",
    "def categorize_tags(tag_embeddings, diversity_embedding, discipline_embedding, tags):\n",
    "    # Generate embeddings for tags, diversity area, and discipline\n",
    "    tag_embeddings = model.encode(selected_tags)\n",
    "    diversity_embedding = model.encode([desired_diversity])\n",
    "    discipline_embedding = model.encode([overarching_discipline])\n",
    "    categorized_tags = {}\n",
    "    for i, tag in enumerate(tags):\n",
    "        # Calculate cosine similarity to both diversity and discipline\n",
    "        diversity_sim = cosine_similarity([tag_embeddings[i]], diversity_embedding)[0][0]\n",
    "        discipline_sim = cosine_similarity([tag_embeddings[i]], discipline_embedding)[0][0]\n",
    "\n",
    "        # Categorize based on higher similarity\n",
    "        if diversity_sim > discipline_sim:\n",
    "            categorized_tags[tag] = 'Diversity'\n",
    "        elif discipline_sim > diversity_sim:\n",
    "            categorized_tags[tag] = 'Discipline'\n",
    "        else:\n",
    "            categorized_tags[tag] = 'Neither'\n",
    "\n",
    "    return categorized_tags\n",
    "\n",
    "# Categorize tags\n",
    "categorized_tags = categorize_tags(tag_embeddings, diversity_embedding, discipline_embedding, selected_tags)\n",
    "print(categorized_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e1cbd8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha (overarching discipline): ['racial segregation', 'Southern Gothic', 'southern life', 'racial injustice', 'laws', 'legal stories'] \n",
      " beta (desired diversity): ['mob mentality', 'class', 'loss of innocence', 'domestic fiction'] \n",
      " gamma (neither): []\n"
     ]
    }
   ],
   "source": [
    "cat_alpha, cat_beta, cat_gamma = [], [], []\n",
    "for tag in selected_tags:\n",
    "    if categorized_tags[tag] == 'Discipline':\n",
    "        cat_alpha.append(tag)\n",
    "    elif categorized_tags[tag] == 'Diversity':\n",
    "        cat_beta.append(tag)\n",
    "    else:\n",
    "        cat_gamma.append(tag)\n",
    "        \n",
    "print(\"alpha (overarching discipline):\", cat_alpha, \n",
    "      \"\\n beta (desired diversity):\", cat_beta,\n",
    "     \"\\n gamma (neither):\", cat_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "02b20c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rao's Entropy (Diversity): 0.17498199820518495\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model for topic embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Define Rao's entropy formula\n",
    "def raos_entropy(cat_alpha, cat_beta):\n",
    "    # Generate embeddings for topics\n",
    "    alpha_embeddings = model.encode(np.unique(cat_alpha))\n",
    "    beta_embeddings = model.encode(np.unique(cat_beta))\n",
    "\n",
    "    # Full list of syllabus topics\n",
    "    syllabus_topics = cat_alpha + cat_beta\n",
    "\n",
    "    # Calculate proportions (p_i and p_j)\n",
    "    topic_counts = Counter(syllabus_topics)\n",
    "    total_topics = len(syllabus_topics)\n",
    "    p_alpha = np.array([topic_counts[topic] / total_topics for topic in cat_alpha])\n",
    "    p_beta = np.array([topic_counts[topic] / total_topics for topic in cat_beta])\n",
    "    entropy = 0.0\n",
    "    # Calculate pairwise cosine distances between topics\n",
    "    distance_matrix = cosine_distances(alpha_embeddings, beta_embeddings)\n",
    "    \n",
    "    # Sum over all topic pairs\n",
    "    for i in range(len(cat_alpha)):\n",
    "        for j in range(len(cat_beta)):\n",
    "            entropy += p_alpha[i] * p_beta[j] * distance_matrix[i, j]\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "# Calculate diversity using Rao's entropy\n",
    "entropy = raos_entropy(cat_alpha, cat_beta)\n",
    "print(f\"Rao's Entropy (Diversity): {entropy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7b3c5f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "z = cat_alpha.extend(['abc'])\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e0c59a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_a, cat_b, cat_c = ['a'], ['b'], ['c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce03e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf1af5e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the+lord+of+the+tings'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Define some APA syllabus labels for grouping\n",
    "# apa_labels = {\n",
    "#     \"Gentrification\": \"Social Identities\",\n",
    "#     \"Housing policy\": \"Social Identities\",\n",
    "#     \"Latin Americans\": \"World\",\n",
    "#     \"Puerto Ricans\": \"World\",\n",
    "#     # Add more mappings as needed\n",
    "# }\n",
    "\n",
    "# def group_tags(tags):\n",
    "#     \"\"\"Group book tags based on APA syllabus labels.\"\"\"\n",
    "#     grouped_tags = {label: [] for label in apa_labels.values()}\n",
    "#     for tag in tags:\n",
    "#         category = apa_labels.get(tag, None)\n",
    "#         if category:\n",
    "#             grouped_tags[category].append(tag)\n",
    "#     return grouped_tags\n",
    "\n",
    "# # Example grouping\n",
    "# grouped_tags = group_tags(tags)\n",
    "# print(f\"Grouped Tags: {grouped_tags}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83ea1c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped Tags: {'African and African-American Philosophy': [], 'Feminist Philosophy': [], 'Latin American and Hispanic Philosophy': [], 'Asian and Asian-American Philosophy': [], 'Social and Political Philosophy': [], 'Ethics': [], 'Philosophy of Religion': [], 'Metaphysics and Epistemology': [], 'Other': ['The Lord of the Rings', 'Fiction', 'Ficción', 'English Fantasy fiction', 'Ficción fantástica inglesa', 'Fantasy fiction', 'Open Library Staff Picks', 'Middle Earth (Imaginary place)', 'Fiction, fantasy, epic', 'Middle earth (imaginary place), fiction', 'Baggins, frodo (fictitious character), fiction', 'Gandalf (fictitious character), fiction', 'British and irish fiction (fictional works by one author)', 'English literature', 'Frodo Baggins (Fictitious character)', 'Baggins, bilbo (fictitious character), fiction', 'Fiction, fantasy, general', 'English language', 'Fiction, media tie-in', 'Gift books', 'Quests (Expeditions)', 'Wizards', 'Terre du Milieu (Lieu imaginaire)', 'Romans, nouvelles', 'Quête', 'Lord of the rings (Tolkien, J.R.R.)', 'Филологические науки. Художественная литература -- Великобритания -- Английская литература -- с 1945 г. -- Произведения художественной литературы -- Художественная проза -- Романы. Повести. Рассказы -- Фантастические и мистические романы, повести, рассказы. Романы ужасов', 'Baggins, frodo', 'Middle earth (imaginary place)--fiction', 'Fantasy fiction, english', 'Pr6039.o32 l6 2005', '823/.912', 'Fiction in English', 'Hobbits (Fictitious characters)', 'Hobbits (Personnages fictifs)', 'Large type books']}\n",
      "Rao's Entropy: 0.0\n",
      "Recommended Books: ['The Autobiography of Malcolm X', 'Harry Potter', 'The Lord of the Rings', '1984']\n"
     ]
    }
   ],
   "source": [
    "# Define APA syllabus labels based on the categories from the site\n",
    "apa_labels = {\n",
    "    \"African and African-American Philosophy\": [\n",
    "        \"African philosophy\", \"African-American philosophy\", \"Black studies\"\n",
    "    ],\n",
    "    \"Feminist Philosophy\": [\n",
    "        \"Feminism\", \"Women's studies\", \"Gender studies\", \"Queer theory\"\n",
    "    ],\n",
    "    \"Latin American and Hispanic Philosophy\": [\n",
    "        \"Latin American philosophy\", \"Hispanic studies\", \"Chicano studies\"\n",
    "    ],\n",
    "    \"Asian and Asian-American Philosophy\": [\n",
    "        \"Asian philosophy\", \"Eastern philosophy\", \"Chinese philosophy\", \"Indian philosophy\", \"Japanese philosophy\"\n",
    "    ],\n",
    "    \"Social and Political Philosophy\": [\n",
    "        \"Gentrification\", \"Housing policy\", \"Social justice\", \"Political science\", \"Democracy\", \"Marxism\"\n",
    "    ],\n",
    "    \"Ethics\": [\n",
    "        \"Ethics\", \"Moral philosophy\", \"Bioethics\", \"Applied ethics\", \"Virtue ethics\"\n",
    "    ],\n",
    "    \"Philosophy of Religion\": [\n",
    "        \"Religion\", \"Theology\", \"Religious studies\", \"Spirituality\"\n",
    "    ],\n",
    "    \"Metaphysics and Epistemology\": [\n",
    "        \"Metaphysics\", \"Epistemology\", \"Ontology\", \"Knowledge theory\"\n",
    "    ],\n",
    "    # Add more categories as needed\n",
    "}\n",
    "\n",
    "def group_tags(tags):\n",
    "    \"\"\"Group book tags based on APA syllabus labels.\"\"\"\n",
    "    grouped_tags = {category: [] for category in apa_labels.keys()}\n",
    "    other_tags = []\n",
    "    for tag in tags:\n",
    "        found = False\n",
    "        for category, keywords in apa_labels.items():\n",
    "            if tag.lower() in [keyword.lower() for keyword in keywords]:\n",
    "                grouped_tags[category].append(tag)\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            other_tags.append(tag)\n",
    "    grouped_tags[\"Other\"] = other_tags\n",
    "    return grouped_tags\n",
    "\n",
    "# Example grouping\n",
    "grouped_tags = group_tags(tags)\n",
    "print(f\"Grouped Tags: {grouped_tags}\")\n",
    "\n",
    "def rao_entropy(tag_groups):\n",
    "    \"\"\"Calculate Rao's entropy based on tag groups.\"\"\"\n",
    "    tag_counts = [len(tags) for tags in tag_groups.values() if tags]\n",
    "    probabilities = np.array(tag_counts) / sum(tag_counts)\n",
    "    distances = squareform(pdist(np.eye(len(probabilities)), metric='euclidean'))\n",
    "    return np.dot(probabilities, np.dot(distances, probabilities))\n",
    "\n",
    "# Example calculation\n",
    "diversity_score = rao_entropy(grouped_tags)\n",
    "print(f\"Rao's Entropy: {diversity_score}\")\n",
    "\n",
    "def ia_select(books, lambda_value=0.5):\n",
    "    \"\"\"Balance utility and diversity to recommend new books.\"\"\"\n",
    "    # Simulate utility and diversity scores for each book\n",
    "    utilities = np.random.rand(len(books))\n",
    "    diversities = np.random.rand(len(books))\n",
    "    total_scores = utilities + lambda_value * diversities\n",
    "    # Recommend top-k books based on total score\n",
    "    recommended_books = [book for _, book in sorted(zip(total_scores, books), reverse=True)]\n",
    "    return recommended_books[:5]  # Recommend top-5 books\n",
    "\n",
    "# Example recommendation\n",
    "books = [\"The Lord of the Rings\", \"1984\", \"The Autobiography of Malcolm X\",\n",
    "         \"Harry Potter\"]\n",
    "recommendations = ia_select(books)\n",
    "print(f\"Recommended Books: {recommendations}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d127ed54",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (1,1) and (0,) not aligned: 1 (dim 1) != 0 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     19\u001b[0m syllabus \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe Lord of the Rings\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnother Book\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYet Another Book\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 20\u001b[0m diversity_scores, recommendations \u001b[38;5;241m=\u001b[39m full_pipeline(syllabus)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDiversity Scores: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdiversity_scores\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecommendations: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecommendations\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[17], line 11\u001b[0m, in \u001b[0;36mfull_pipeline\u001b[0;34m(syllabus)\u001b[0m\n\u001b[1;32m      8\u001b[0m     all_tags\u001b[38;5;241m.\u001b[39mappend(grouped)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Step 2: Calculate diversity score\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m diversity_scores \u001b[38;5;241m=\u001b[39m [rao_entropy(tags) \u001b[38;5;28;01mfor\u001b[39;00m tags \u001b[38;5;129;01min\u001b[39;00m all_tags]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Step 3: Recommend new books based on IA-Select\u001b[39;00m\n\u001b[1;32m     14\u001b[0m recommendations \u001b[38;5;241m=\u001b[39m ia_select(syllabus)\n",
      "Cell \u001b[0;32mIn[17], line 11\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m     all_tags\u001b[38;5;241m.\u001b[39mappend(grouped)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Step 2: Calculate diversity score\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m diversity_scores \u001b[38;5;241m=\u001b[39m [rao_entropy(tags) \u001b[38;5;28;01mfor\u001b[39;00m tags \u001b[38;5;129;01min\u001b[39;00m all_tags]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Step 3: Recommend new books based on IA-Select\u001b[39;00m\n\u001b[1;32m     14\u001b[0m recommendations \u001b[38;5;241m=\u001b[39m ia_select(syllabus)\n",
      "Cell \u001b[0;32mIn[16], line 55\u001b[0m, in \u001b[0;36mrao_entropy\u001b[0;34m(tag_groups)\u001b[0m\n\u001b[1;32m     53\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(tag_counts) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28msum\u001b[39m(tag_counts)\n\u001b[1;32m     54\u001b[0m distances \u001b[38;5;241m=\u001b[39m squareform(pdist(np\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;28mlen\u001b[39m(probabilities)), metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdot(probabilities, np\u001b[38;5;241m.\u001b[39mdot(distances, probabilities))\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (1,1) and (0,) not aligned: 1 (dim 1) != 0 (dim 0)"
     ]
    }
   ],
   "source": [
    "def full_pipeline(syllabus):\n",
    "    \"\"\"Full pipeline to process syllabus and recommend diverse books.\"\"\"\n",
    "    # Step 1: Fetch tags for each book\n",
    "    all_tags = []\n",
    "    for book in syllabus:\n",
    "        tags = get_book_tags(book)\n",
    "        grouped = group_tags(tags)\n",
    "        all_tags.append(grouped)\n",
    "    \n",
    "    # Step 2: Calculate diversity score\n",
    "    diversity_scores = [rao_entropy(tags) for tags in all_tags]\n",
    "    \n",
    "    # Step 3: Recommend new books based on IA-Select\n",
    "    recommendations = ia_select(syllabus)\n",
    "    \n",
    "    return diversity_scores, recommendations\n",
    "\n",
    "# Example usage\n",
    "syllabus = [\"The Lord of the Rings\", \"Another Book\", \"Yet Another Book\"]\n",
    "diversity_scores, recommendations = full_pipeline(syllabus)\n",
    "print(f\"Diversity Scores: {diversity_scores}\")\n",
    "print(f\"Recommendations: {recommendations}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
